{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9169ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "# Data Exploration\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#random\n",
    "import random\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "# Pre processing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Metric Calculations\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Synthetic data generation library\n",
    "# import sdv\n",
    "\n",
    "# Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Sampling Libraries\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Concept Drift Detection tools\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Custom Synthetic Data Creation Functions using SDV\n",
    "import synth_data_lib_v2_extra_drift\n",
    "\n",
    "# supress warning outputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 2 Bank Fraud Detection\n",
    "# Stephen Montgomery (smontg12@depaul.edu)\n",
    "# John Bolgert (jbolgert@depaul.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1521705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e571699",
   "metadata": {},
   "source": [
    "# Pre Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd602f99",
   "metadata": {},
   "source": [
    "### Obtain median cost of Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3caf005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Meidan Fraud Transfer amount is : 34.70264688784764\n"
     ]
    }
   ],
   "source": [
    "# store median fraud to be used later in optimization as the cost of FN\n",
    "median_fraud = df[(df['fraud_bool']==1)&(df['intended_balcon_amount']>0)]['intended_balcon_amount'].median()\n",
    "print('The Meidan Fraud Transfer amount is :', median_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdb06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use months 7 for base data and 6 for adding in noise each day of the simulation\n",
    "df_use = df[(df['month']==7) | (df['month']==6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d0ccda",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb8a88",
   "metadata": {},
   "source": [
    "### Addressing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd70c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income  discretize feature into 2 bins\n",
      "prev_address_months_count  discretize feature into 5 bins\n",
      "current_address_months_count  fill using median\n",
      "customer_age  discretize feature into 2 bins\n",
      "intended_balcon_amount  discretize feature into 5 bins\n",
      "velocity_6h  fill using median\n",
      "date_of_birth_distinct_emails_4w  discretize feature into 5 bins\n",
      "[-1, 7, 15, 22, 30, 37]\n",
      "credit_risk_score  fill using median\n",
      "bank_months_count  discretize feature into 5 bins\n",
      "[-2, 7, 13, 20, 26, 33]\n",
      "proposed_credit_limit  discretize feature into 2 bins\n",
      "session_length_in_minutes  fill using median\n",
      "device_distinct_emails_8w  convert numeric to categorical feature\n"
     ]
    }
   ],
   "source": [
    "### Discretize each feature that has greater than 10% missing values for fraud \n",
    "features = list(df_use.columns)\n",
    "total_rows = len(df_use)\n",
    "total_fraud = len(df_use[df_use['fraud_bool']==1])\n",
    "\n",
    "# Numeric features with less than 25 unqiue values in the base data get discretized into bins if less than 5 they are\n",
    "# directly converted using their assoicated number if larger than they get binned into small large based on median value\n",
    "for col in features:\n",
    "    bins = []\n",
    "    if is_numeric_dtype(df_use[col]):\n",
    "        if col == 'fraud_bool' or col=='month' :\n",
    "            continue\n",
    "        if len(df_use[col].unique())<5 or col=='month':\n",
    "            if len(df_use[df_use[col]<0])>0:\n",
    "                print(col, ' convert numeric to categorical feature')\n",
    "                total_null = len(df_use[df_use[col]<0])\n",
    "                fraud_null = len(df_use[(df_use[col]<0) & (df_use['fraud_bool']==1)])\n",
    "                df_use[col]=df_use[col].astype(str)\n",
    "        elif len(df_use[col].unique())<25:\n",
    "            print(col, ' discretize feature into 2 bins')\n",
    "            # Discretize the feature\n",
    "            med_val = df_use[col].median()\n",
    "            min_val = df_use[col].min()\n",
    "            max_val = df_use[col].max()\n",
    "            bin_labels = ['Small','Large']\n",
    "            \n",
    "            bins = [min_val-1, med_val, max_val]\n",
    "            df_use[col] = pd.cut(df_use[col], bins=bins, labels=bin_labels)\n",
    "            \n",
    "        elif len(df_use[col].unique())<100:\n",
    "            # if less than 100 unqiue vlaues then value gets discretized into 5 bins\n",
    "            print(col, ' discretize feature into 5 bins')\n",
    "            # Discretize the feature\n",
    "            med_val = df_use[col].median()\n",
    "            min_val = df_use[col].min()\n",
    "            max_val = df_use[col].max()\n",
    "            bin_labels = ['bin1','bin2','bin3','bin4','bin5']\n",
    "            \n",
    "            for i in range(6):\n",
    "                if i == 0:\n",
    "                    bins.append(min_val-1)\n",
    "                else: \n",
    "                    bins.append(i*(max_val - min_val)/5)\n",
    "            bins = [round(num) for num in bins]    \n",
    "            print(bins)\n",
    "            df_use[col] = pd.cut(df_use[col], bins=bins, labels=bin_labels)\n",
    "            \n",
    "        else:\n",
    "            if len(df_use[df_use[col]<0])>0:\n",
    "                total_null = len(df_use[df_use[col]<0])\n",
    "                fraud_null = len(df_use[(df_use[col]<0) & (df_use['fraud_bool']==1)])\n",
    "                # discretize the feature if more than 10% fraud values are negative \n",
    "                if fraud_null/total_fraud > .1:\n",
    "                    print(col, ' discretize feature into 5 bins')\n",
    "                    # Discretize the feature\n",
    "                    min_val = df_use[col].min()\n",
    "                    max_val = df_use[col].max()\n",
    "                    bins = [min_val-1]\n",
    "                    bin_labels = ['Missing']\n",
    "                    for i in range(5):\n",
    "                        bins.append(i*max_val/4)\n",
    "                        bin_labels.append('bin_{}'.format(i))\n",
    "                    bin_labels.pop()\n",
    "                    df_use[col] = pd.cut(df_use[col], bins=bins, labels=bin_labels)\n",
    "                else:\n",
    "                    print(col, ' fill using median')\n",
    "                    # Fill negative values with median vallue\n",
    "                    med= df[col].median()\n",
    "                    df_use.loc[df_use[col]<0,col]=med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864c1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the base data into data to be simulated and data to be added to simulated data to represent noise\n",
    "\n",
    "# df add will be used as noise and will not be trained on for the base model\n",
    "df_add = df_use[df_use['month']==6]\n",
    "df_add = df_add[df_add['fraud_bool']==0]\n",
    "\n",
    "# df use will be base data inital model is trained on\n",
    "df_use = df_use[df_use['month']==7]\n",
    "df_use.drop('month',axis=1,inplace=True)\n",
    "df_add.drop('month',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3880773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into dependent vs. independent features\n",
    "y_use = df_use['fraud_bool']\n",
    "x_use = df_use.drop('fraud_bool',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230e8cc",
   "metadata": {},
   "source": [
    "### Categorical Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3880d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Categorical\n",
    "df_dummies = pd.get_dummies(x_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf236c",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b158c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all the columns to range between 0 and 1\n",
    "x = df_dummies.values\n",
    "cols = df_dummies.columns\n",
    "min_max_scaler_base = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler_base.fit_transform(x)\n",
    "x_use_scaled = pd.DataFrame(x_scaled, columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e87529",
   "metadata": {},
   "source": [
    "### Train Test Split portion of overall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4adb40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,  x_test, y_train , y_test = train_test_split(x_use_scaled, y_use, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335a233",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "535b8239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank Feature importance based on their mean decrease in impurity using random forest claissifer\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "features = x_train.columns\n",
    "clf.fit(x_train, y_train)\n",
    "importances = clf.feature_importances_\n",
    "len(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f207c",
   "metadata": {},
   "source": [
    "# Model Metric & Training Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d8f92",
   "metadata": {},
   "source": [
    "### Cost Savings Optimization Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c74725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Custom Preformance Metrics Function\n",
    "def optimize_cost(Cost_FN, Cost_FP,probabilities, y_test):\n",
    "    # function to select optimal probability given the custom loss function and will return optimal prob \n",
    "    # for optimizing cost savings or f1 score\n",
    "    opt_savings = 'NULL'\n",
    "    opt_prob = 'NULL'\n",
    "    opt_f1 = ['NULL','NULL']\n",
    "    # iterate through and store optimal probability for cost savings and F1 Score\n",
    "    for prob in np.arange(.01,1,.01):\n",
    "        predictions = probabilities[:,1].copy()\n",
    "        predictions[predictions>=prob] = 1\n",
    "        predictions[predictions<prob] = 0\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "        savings = cm[1,1]*Cost_FN - cm[0,1]*Cost_FP\n",
    "        f1_score_iter = f1_score(y_test, predictions)\n",
    "        if opt_savings=='NULL':\n",
    "            opt_savings = savings\n",
    "            opt_prob = prob\n",
    "        elif savings > opt_savings:\n",
    "            opt_savings = savings\n",
    "            opt_prob = prob\n",
    "        if opt_f1[0]=='NULL':\n",
    "            opt_f1[0] = f1_score_iter\n",
    "            opt_f1[1] = prob\n",
    "        elif f1_score_iter > opt_f1[0]:\n",
    "            opt_f1[0] = f1_score_iter\n",
    "            opt_f1[1] = prob\n",
    "    # calculate the F1 score given the cost savings optimal probability \n",
    "    predictions = probabilities[:,1]\n",
    "    predictions[predictions>=opt_prob] = 1\n",
    "    predictions[predictions<opt_prob] = 0 \n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, probabilities[:, 1])\n",
    "    return ([opt_prob, opt_savings,f1 ,precision,recall,roc_auc,opt_f1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584cf731",
   "metadata": {},
   "source": [
    "# Model Building & Optimization on Base Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532dd8e5",
   "metadata": {},
   "source": [
    "### Base Models Selected are\n",
    "    1. Gradient Boosting Classifier (All Features ROS .25)\n",
    "    2. Stochastic Gradient Descent Classifier (All Features RUS .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae8391ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimization metrics for custom loss function\n",
    "cost_fp = 10\n",
    "cost_fn = median_fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91576910",
   "metadata": {},
   "source": [
    "### Model building: SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7a478a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting SGDClassifier Optimization\n",
      "Optimal Confidence for savings = 0.81 @ savings = 1306.2117510278113\n",
      "F1 training metric for cost savings optimization =  0.22130013831258644\n",
      "Optimal Confidence for F1 = 0.66 @ f1 score = 0.2803445575567737\n",
      "Seconds Taken to SGDClassifier:  8.040725946426392\n"
     ]
    }
   ],
   "source": [
    "# Find Optimal Feature Sampeling & ratio Amount\n",
    "print('starting SGDClassifier Optimization')\n",
    "start_time = time.time()\n",
    "\n",
    "# optimize parameters \n",
    "SGD = SGDClassifier(loss='log_loss')\n",
    "SGD_parameters = {\n",
    "        'alpha':[.01,.001,.0001],\n",
    "        'max_iter':[200,1000,2000]\n",
    "        }\n",
    "opt_SGD = GridSearchCV(SGD, SGD_parameters)\n",
    "\n",
    "# Apply Sampeling technique\n",
    "oversample = RandomOverSampler(sampling_strategy=.25)\n",
    "x_sampled, y_sampled = oversample.fit_resample(x_train,y_train)\n",
    "\n",
    "# Fit and optimize the model\n",
    "opt_SGD.fit(x_sampled, y_sampled)\n",
    "probabilities_SGD = opt_SGD.predict_proba(x_test)\n",
    "\n",
    "terms = optimize_cost(cost_fn, cost_fp,probabilities_SGD, y_test)\n",
    "probabilities_SGD = opt_SGD.predict_proba(x_test)\n",
    "probabilities_SGD = probabilities_SGD[:,1]\n",
    "\n",
    "\n",
    "print(\"Optimal Confidence for savings = {} @ savings = {}\".format(terms[0], terms[1]))\n",
    "SGD_Savings_proba = terms[0]\n",
    "print(\"F1 training metric for cost savings optimization = \",terms[2])\n",
    "SGD_Savings_F1 = terms[2]\n",
    "print(\"Optimal Confidence for F1 = {} @ f1 score = {}\".format(terms[-1][1], terms[-1][0]))\n",
    "SGD_F1_proba = terms[-1][1]\n",
    "SGD_F1_F1 =  terms[-1][0]\n",
    "print(\"Seconds Taken to SGDClassifier: \", (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f162a9",
   "metadata": {},
   "source": [
    "### Model building: GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfec08b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting GradientBoostingClassifier Optimization\n",
      "Optimal Confidence for savings = 0.92 @ savings = 1639.7249854670495\n",
      "F1 training metric for cost savings optimization =  0.23876404494382023\n",
      "Optimal Confidence for F1 = 0.86 @ f1 score = 0.27755905511811024\n",
      "Seconds Taken to SGDClassifier:  90.84873008728027\n",
      "Seconds Taken to GradientBoostingClassifier:  90.84873008728027\n"
     ]
    }
   ],
   "source": [
    "# Find Optimal Feature Sampeling & ratio Amount\n",
    "print('starting GradientBoostingClassifier Optimization')\n",
    "start_time = time.time()\n",
    "\n",
    "    \n",
    "# optimize parameters \n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC_parameters = {\n",
    "        'n_estimators':[50,100,200],\n",
    "        'max_depth':[2,3,5],\n",
    "        'min_samples_split':[2,10]\n",
    "    }\n",
    "opt_GBC = GridSearchCV(GBC, GBC_parameters)\n",
    "\n",
    "# Apply Sampeling technique\n",
    "undersample = RandomUnderSampler(sampling_strategy=.5)\n",
    "x_sampled, y_sampled = undersample.fit_resample(x_train,y_train)\n",
    "\n",
    "# Fit and optimize the model\n",
    "opt_GBC.fit(x_sampled, y_sampled)\n",
    "probabilities_GBC = opt_GBC.predict_proba(x_test)\n",
    "terms = optimize_cost(cost_fn, cost_fp,probabilities_GBC, y_test)\n",
    "probabilities_GBC = opt_GBC.predict_proba(x_test)\n",
    "probabilities_GBC = probabilities_GBC[:,1]\n",
    "\n",
    "print(\"Optimal Confidence for savings = {} @ savings = {}\".format(terms[0], terms[1]))\n",
    "GBC_Savings_proba = terms[0]\n",
    "print(\"F1 training metric for cost savings optimization = \",terms[2])\n",
    "GBC_Savings_F1 = terms[2]\n",
    "print(\"Optimal Confidence for F1 = {} @ f1 score = {}\".format(terms[-1][1], terms[-1][0]))\n",
    "GBC_F1_proba = terms[-1][1]\n",
    "GBC_F1_F1 =  terms[-1][0]\n",
    "print(\"Seconds Taken to SGDClassifier: \", (time.time() - start_time))\n",
    "\n",
    "print(\"Seconds Taken to GradientBoostingClassifier: \", (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144fa50a",
   "metadata": {},
   "source": [
    "# Monte Carlo Simulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21249a",
   "metadata": {},
   "source": [
    "### Simulation Outline\n",
    "    1. Simulation has 2 for loops outer loop is number of complete iterations to run inner loop is number of simulated days in one iteration\n",
    "    2. Inner loop should creates 20 new datasets in each loop with each day having 10% new data injected into the non fraud simulated data and a complete distribution shift to fraud on days 4, 9, & 14. Each day simualtes data from the previous day before noise is added to it. \n",
    "    3. new data is statistically similar and sampeled from the base data\n",
    "    4. each day each of the 9 strategies per model predicts on the data to get results than tries to detect concept drift given its strategy. If it detects concept drift it retrains given its retrain strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897e63c",
   "metadata": {},
   "source": [
    "### Concept Detection & Retraining Stragies Outline (total 9 strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed462d",
   "metadata": {},
   "source": [
    "### Defining Concept Drift Detection Strategies\n",
    "\n",
    "    1. Retrain once testing(New Day) f1 score drops below its f1 training or re-training metric. \n",
    "    2. Retrain using statistical signficance tsest Kolmogorov-Smirnov test for goodness of fit on each of the continous numeric features and chi Squared test on categorical features. In cases where goodness of fit does not work we use a distance measurment and compare that new days distance measurment to the previous days means plus 1 standrd deviation distance measurments. if the distance excedes this number we consider the distribution changes. If more than a certain threshold of features show significant shift retrain. \n",
    "    3. Retrain using Population Stability Index on models predicted probability and retrain if PSI is greater than .1\n",
    "\n",
    "### Defining Retraining Strategies\n",
    "\n",
    "    1. Retrain on current days data using (RUS mintority class @ 50%)\n",
    "    2. Retrain on past up to past 3 days data using (RUS mintority class @ 50%)\n",
    "    3. Retrain on current days data using (RUS mintority class @ 50%) + add previous past 2 days data with undersampeling less than 50% giving more emphasis to current days data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ac3cc",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ace0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes dataframe and preforms pre processing to get dummy varibels and ensure all neccesary columns are present\n",
    "def pre_processing(data_frame,req_cols,model_dict_obj=None,retrain=0):\n",
    "    y = data_frame['fraud_bool']\n",
    "    x = data_frame.drop(['fraud_bool','day'],axis=1).copy()\n",
    "\n",
    "    # Check for days or fraud_bool in req cols\n",
    "    for col in req_cols:\n",
    "        if col in ['day','fraud_bool']:\n",
    "            req_cols.remove(col)\n",
    "            \n",
    "    # Convert Categorical\n",
    "    df_dummies = pd.get_dummies(x)\n",
    "\n",
    "    \n",
    "    x_vals = df_dummies.values\n",
    "    cols = df_dummies.columns\n",
    "    # if retrain is zero use model objects min max class\n",
    "    if retrain == 0:\n",
    "        min_max_scaler = model_dict_obj[11]\n",
    "        x_scaled = min_max_scaler.fit_transform(x_vals)\n",
    "        x_use_scaled = pd.DataFrame(x_scaled, columns = cols)\n",
    "    \n",
    "    # if we are retraining create a new min man object to store in the model object \n",
    "    else:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        x_scaled = min_max_scaler.fit_transform(x_vals)\n",
    "        x_use_scaled = pd.DataFrame(x_scaled, columns = cols)\n",
    "    \n",
    "    # find columns that are not in new data that are in base data and add them to the base data\n",
    "    add_cols = list(set(req_cols) - set(cols))\n",
    "    subtract_cols = list(set(cols) - set(req_cols))\n",
    "    if len(add_cols) > 0:\n",
    "        for new_col in add_cols:\n",
    "            if new_col == 'day':\n",
    "                continue\n",
    "            else:\n",
    "                x_use_scaled[new_col]=0\n",
    "    # Remove columns that are not in base data that are in new data\n",
    "    if len(subtract_cols) > 0:\n",
    "        for new_col in subtract_cols:\n",
    "            if new_col == 'day':\n",
    "                continue\n",
    "            else:\n",
    "                x_use_scaled = x_use_scaled.drop(new_col,axis=1)\n",
    "    # Return the object used for min max scaling as well as the pre-processed x and y values \n",
    "    x_use_scaled = x_use_scaled[list(req_cols)]\n",
    "    return(x_use_scaled, y, min_max_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b06a62",
   "metadata": {},
   "source": [
    "### Retraining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6be29d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to retrain on simulated data given the strategies retrain method\n",
    "def re_training(data,sampeling_ratio,strategy,base_model,req_cols):\n",
    "    max_day = data['day'].max()\n",
    "\n",
    "    # retrain on 1 days worth of data\n",
    "    if strategy==1:  \n",
    "        \n",
    "        re_train = data[data['day']==max_day]\n",
    "        \n",
    "        # Pre processing for new data\n",
    "        x_train,y_train,min_max_scaler = pre_processing(re_train,req_cols,retrain=1)\n",
    "        \n",
    "        # sampeling \n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampeling_ratio)\n",
    "        x_sampled, y_sampled = undersample.fit_resample(x_train,y_train)\n",
    "        base_model.fit(x_sampled, y_sampled)\n",
    "        return(base_model,x_train,y_train,min_max_scaler)\n",
    "    \n",
    "    # retrain on 3 days worth of data\n",
    "    elif strategy==2: \n",
    "        \n",
    "        \n",
    "        re_train = data[data['day']>max_day-3]\n",
    "        # Pre processing for new data\n",
    "        x_train,y_train,min_max_scaler = pre_processing(re_train,req_cols,retrain=1)\n",
    "        \n",
    "        # Sampeling \n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampeling_ratio)\n",
    "        x_sampled, y_sampled = undersample.fit_resample(x_train,y_train)\n",
    "        base_model.fit(x_sampled, y_sampled)\n",
    "        return(base_model,x_train,y_train,min_max_scaler)\n",
    "    # retrain on 3 days worth of data but give more weight to most recent day\n",
    "    else:\n",
    "        \n",
    "        \n",
    "        re_train = data[data['day']>(max_day-3)]\n",
    "        days = re_train['day']\n",
    "        # Pre processing for new data\n",
    "        \n",
    "        x_train,y_train,min_max_scaler = pre_processing(re_train,req_cols,retrain=1)\n",
    "        \n",
    "        # sampeling for new days data\n",
    "        x_train['fraud_bool'] = list(y_train)\n",
    "        post_proc_data = x_train.copy()\n",
    "        x_train.drop('fraud_bool',axis=1,inplace=True)\n",
    "        \n",
    "        \n",
    "        post_proc_data['day']= list(days)\n",
    "        \n",
    "        \n",
    "        re_train = post_proc_data[post_proc_data['day']==max_day]\n",
    "        \n",
    "        x_train=re_train.drop(['fraud_bool','day'],axis=1)\n",
    "        y_train=re_train['fraud_bool']\n",
    "        \n",
    "        \n",
    "        oversample = RandomUnderSampler(sampling_strategy=sampeling_ratio)\n",
    "        x_sampled_curr, y_sampled_curr = oversample.fit_resample(x_train,y_train)\n",
    "        \n",
    "        # samepling for other days that is not new days data\n",
    "        add_data = post_proc_data[post_proc_data['day']!=max_day]\n",
    "        x_train=add_data.drop(['fraud_bool','day'],axis=1)\n",
    "        y_train=add_data['fraud_bool']\n",
    "        \n",
    "        fraud_curr_ratio = y_train.sum()/len(y_train)\n",
    "        \n",
    "        # use lower sampeling ratio for these days to give more wieght to newer data\n",
    "        if (fraud_curr_ratio/(1-fraud_curr_ratio))>(sampeling_ratio/3):\n",
    "            x_sampled_past, y_sampled_past = x_train, y_train\n",
    "        else:\n",
    "            oversample = RandomUnderSampler(sampling_strategy=sampeling_ratio/3)\n",
    "            x_sampled_past, y_sampled_past = oversample.fit_resample(x_train,y_train)\n",
    "        \n",
    "        \n",
    "        x_sampled =pd.concat([x_sampled_curr,x_sampled_past],ignore_index=True)\n",
    "        y_sampled = pd.concat([y_sampled_curr,y_sampled_past],ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        base_model.fit(x_sampled, y_sampled)\n",
    "        return(base_model,x_train,y_train,min_max_scaler)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c4c47",
   "metadata": {},
   "source": [
    "### Create Concept Drift Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f816f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions below used to detect concept drift then retrain using strategies detection and retrain methods\n",
    "\n",
    "# function compares current days metric to the training metric and retrains if it is lower by a certain threshold\n",
    "def cd_detection_metric_comparison(current_metric, training_metric, threshold,model_dictionary,data,day,sim,concept_drift,Cost_FN, Cost_FP,sampeling_ratio=.5):\n",
    "    CD_tracked = ['Day','Simulation','CD_Detected','CD_Occured']\n",
    "    \n",
    "    # for each day check if the new days metric is below the training training metric minus the threshold\n",
    "    if ((training_metric - current_metric) > threshold) and (day > 2):\n",
    "        print('        Drift Detected using: Metric Comparision')\n",
    "        print('        Current_Metric: ',current_metric)\n",
    "        print('        Train Metric: ',training_metric)\n",
    "        \n",
    "        metric_df = pd.DataFrame([[day,sim,1,concept_drift]], columns=CD_tracked)\n",
    "        new_df = pd.concat([model_dictionary[9], metric_df],ignore_index=True)\n",
    "        \n",
    "        # if concept drift detected retrain the model and update the model object with neccessary values\n",
    "        new_model, x, y,min_max_scaler = re_training(data,sampeling_ratio,model_dictionary[1],model_dictionary[0],model_dictionary[10])\n",
    "        \n",
    "        \n",
    "        # Compute Training metric with new model\n",
    "        preds = new_model.predict(x)\n",
    "        pred_prob = new_model.predict_proba(x)\n",
    "        res = optimize_cost(Cost_FN, Cost_FP, pred_prob, y)\n",
    "        #new_train_metric = f1_score(y, preds)\n",
    "        new_train_metric = res[2]\n",
    "        new_opt_prob = res[0]\n",
    "        \n",
    "        # udpate the neccessary model values \n",
    "        model_dictionary[9] = new_df\n",
    "        model_dictionary[0] = new_model\n",
    "        model_dictionary[3] = new_train_metric\n",
    "        model_dictionary[7] = new_opt_prob\n",
    "        model_dictionary[11] = min_max_scaler\n",
    "        model_dictionary[5] = pred_prob[:,1]\n",
    "        return (model_dictionary)\n",
    "    else:\n",
    "        # if no concept drift detected modify concept drift data tracking metrics and return unchanged model object\n",
    "        metric_df = pd.DataFrame([[day,sim,0,concept_drift]], columns=CD_tracked)\n",
    "        new_df = pd.concat([model_dictionary[9], metric_df],ignore_index=True)\n",
    "        model_dictionary[9] = new_df\n",
    "        return(model_dictionary)\n",
    "\n",
    "# function compares the fraud distribution of new day to previous days to detect concept drift   \n",
    "def cd_detection_Feature_distribution(all_data, threshold_criteria,model_dictionary,day,sim,concept_drift,Cost_FN, Cost_FP,sampeling_ratio=.5):\n",
    "    CD_tracked = ['Day','Simulation','CD_Detected','CD_Occured']\n",
    "    features_used = all_data.drop(['day','fraud_bool'],axis=1,errors='ignore').columns\n",
    "    \n",
    "    curr_day = all_data['day'].max()\n",
    "    # divide data into current days fraud distribution Vs. previous days fraud distribution\n",
    "    base_data = all_data[(all_data['day']<curr_day) & (all_data['day']>=model_dictionary[12]) & (all_data['fraud_bool']==1)]\n",
    "    new_data  = all_data[(all_data['day']==curr_day) & (all_data['fraud_bool']==1)]\n",
    "    Features_detected = 0\n",
    "    # first 2 days after first day are used to store probabity distribution distances for later comparision\n",
    "    if day < 3 and day!=0:\n",
    "        for col in features_used:\n",
    "            # if col is numeric but less than 15 unique values treat it as a categorical feature\n",
    "            if is_numeric_dtype(all_data[col]) and len(all_data[col].unique())<15:\n",
    "                \n",
    "                # concert feature into frequency values for each value of the feature then combine them and fill na with 0\n",
    "                s1 = base_data.value_counts(col)/len(base_data)\n",
    "                s2 = new_data.value_counts(col)/len(new_data)\n",
    "                \n",
    "                freq_df = pd.concat([s1, s2], axis=1,ignore_index=True)\n",
    "                freq_df = freq_df.fillna(0)\n",
    "                expected = freq_df.iloc[:,0]\n",
    "                observed = freq_df.iloc[:,1]\n",
    "                \n",
    "                # calculate the distance between the current days probability distribution and past days then store in \n",
    "                # model object dataframe \n",
    "                prob_dist = distance.jensenshannon(expected, observed, axis=0)\n",
    "                model_dictionary[13].loc[day,col] = prob_dist\n",
    "            elif is_numeric_dtype(all_data[col])==False:\n",
    "                \n",
    "                s1 = base_data.value_counts(col)/len(base_data)\n",
    "                s2 = new_data.value_counts(col)/len(new_data)\n",
    "                freq_df = pd.concat([s1, s2], axis=1,ignore_index=True)\n",
    "                freq_df = freq_df.fillna(0)\n",
    "                expected = freq_df.iloc[:,0]\n",
    "                observed = freq_df.iloc[:,1]\n",
    "\n",
    "                prob_dist = distance.jensenshannon(expected, observed, axis=0)\n",
    "                model_dictionary[13].loc[day,col] = prob_dist\n",
    "    # third day is when we allow retrain to begin        \n",
    "    if day > 2:\n",
    "        feat_with_drift = []\n",
    "        for col in features_used:\n",
    "            \n",
    "            if is_numeric_dtype(all_data[col]):\n",
    "                \n",
    "                # For Categorical need to check that all values are present in base data are in new_data\n",
    "                if len(all_data[col].unique())<15:\n",
    "                    s1 = base_data.value_counts(col)/len(base_data)\n",
    "                    s2 = new_data.value_counts(col)/len(new_data)\n",
    "                    \n",
    "                    # convert categorical feature into frequency distributions\n",
    "                    freq_df = pd.concat([s1, s2], axis=1,ignore_index=True)\n",
    "                    freq_df = freq_df.fillna(0)\n",
    "                    expected = freq_df.iloc[:,0]\n",
    "                    observed = freq_df.iloc[:,1]\n",
    "\n",
    "                    prob_dist = distance.jensenshannon(expected, observed, axis=0)\n",
    "                    model_dictionary[13].loc[day,col] = prob_dist\n",
    "                    \n",
    "                    pct_dif = abs(freq_df.iloc[:,0].sum() - freq_df.iloc[:,1].sum())\n",
    "                    \n",
    "                    # if observed or expected contains 0 or the sums of observed and expected are diffrent \n",
    "                    # use distance measurment to detect drift \n",
    "                    if freq_df.isin([0]).any().any() or pct_dif > .00005:\n",
    "                        \n",
    "                        avg = model_dictionary[13][model_dictionary[13]['CD_Detected']==0][col].mean()\n",
    "                        std = model_dictionary[13][model_dictionary[13]['CD_Detected']==0][col].std()\n",
    "                        \n",
    "                        # if new days distance metric is greater than the average * 1.5 stds we consider it drifted \n",
    "                        if prob_dist > (avg + std*1.5):\n",
    "                            feat_with_drift.append(col)\n",
    "                            Features_detected+=1\n",
    "                    # else we use goodness of fit test to see if feature has dirfted \n",
    "                    else:\n",
    "                        p_val = stats.chisquare(f_obs=observed, f_exp=expected).pvalue\n",
    "                        \n",
    "                        if p_val< .05:\n",
    "                            feat_with_drift.append(col)\n",
    "                            Features_detected+=1\n",
    "                else:\n",
    "                    \n",
    "                    p_val = stats.ks_2samp(base_data[col], new_data[col]).pvalue\n",
    "                    \n",
    "                    if p_val< .05:\n",
    "                        feat_with_drift.append(col)\n",
    "                        Features_detected+=1\n",
    "            # if feautre is not numeric use distance or chi squared test goodness of fit for feature drift detection\n",
    "            else:\n",
    "                s1 = base_data.value_counts(col)/len(base_data)\n",
    "                s2 = new_data.value_counts(col)/len(new_data)\n",
    "                freq_df = pd.concat([s1, s2], axis=1,ignore_index=True)\n",
    "                freq_df = freq_df.fillna(0)\n",
    "                expected = freq_df.iloc[:,0]\n",
    "                observed = freq_df.iloc[:,1]\n",
    "                \n",
    "                pct_dif = abs(freq_df.iloc[:,0].sum() - freq_df.iloc[:,1].sum())\n",
    "                \n",
    "                prob_dist = distance.jensenshannon(expected, observed, axis=0)\n",
    "                model_dictionary[13].loc[day,col] = prob_dist\n",
    "                \n",
    "                if freq_df.isin([0]).any().any() or pct_dif > .00005:\n",
    "                    avg = model_dictionary[13][model_dictionary[13]['CD_Detected']==0][col].mean()\n",
    "                    std = model_dictionary[13][model_dictionary[13]['CD_Detected']==0][col].std()\n",
    "                    \n",
    "                    if prob_dist > (avg + std*1.5):\n",
    "                        feat_with_drift.append(col)\n",
    "                        Features_detected+=1       \n",
    "                else:\n",
    "                    p_val = stats.chisquare(f_obs=observed, f_exp=expected).pvalue\n",
    "                    if p_val< .05:\n",
    "                        feat_with_drift.append(col)\n",
    "                        Features_detected+=1\n",
    "    \n",
    "    # if there are more than the theshold of features detected to have drift preform retraining\n",
    "    if (Features_detected> threshold_criteria) and (day > 2):\n",
    "        print('        Drift Detected using: Feature Comparision')\n",
    "        #print('        Features that drifted', feat_with_drift)\n",
    "        metric_df = pd.DataFrame([[day,sim,1,concept_drift]], columns=CD_tracked)\n",
    "        new_df = pd.concat([model_dictionary[9], metric_df],ignore_index=True)\n",
    "        \n",
    "        # retrain and return model and sclaer object used to retrain\n",
    "        new_model,x,y,min_max_scaler = re_training(all_data,sampeling_ratio,model_dictionary[1],model_dictionary[0],model_dictionary[10])\n",
    "        \n",
    "        # obtain new model object values and assign them to the model object\n",
    "        preds = new_model.predict(x)\n",
    "        pred_prob = new_model.predict_proba(x)\n",
    "        res = optimize_cost(Cost_FN, Cost_FP, pred_prob, y)\n",
    "        #new_train_metric = f1_score(y, preds)\n",
    "        new_train_metric = res[2]\n",
    "        new_opt_prob = res[0]\n",
    "        \n",
    "        model_dictionary[9] = new_df\n",
    "        model_dictionary[0] = new_model\n",
    "        model_dictionary[3] = new_train_metric\n",
    "        model_dictionary[5] = pred_prob[:,1]\n",
    "        model_dictionary[7] = new_opt_prob\n",
    "        model_dictionary[11] = min_max_scaler\n",
    "        model_dictionary[12] = day\n",
    "        model_dictionary[13].loc[day,'CD_Detected'] = 1\n",
    "        return(model_dictionary)\n",
    "    else:\n",
    "        metric_df = pd.DataFrame([[day,sim,0,concept_drift]], columns=CD_tracked)\n",
    "        new_df = pd.concat([model_dictionary[9], metric_df],ignore_index=True)\n",
    "        model_dictionary[9] = new_df\n",
    "        model_dictionary[13].loc[day,'CD_Detected'] = 0\n",
    "        return(model_dictionary)\n",
    "\n",
    "# function calculates the Population Stability index and retrains if it is higher than a certain theshold\n",
    "def cd_PSI(training_prob_dist, new_prob_dist, PSI_Threshold,model_dictionary,data,day,sim,concept_drift,Cost_FN, Cost_FP,sampeling_ratio=.5):\n",
    "    CD_tracked = ['Day','Simulation','CD_Detected','CD_Occured']\n",
    "    \n",
    "    prob_df = pd.DataFrame()\n",
    "    prob_df_new = pd.DataFrame()\n",
    "    # split the training fraud propbability and new days fraud probability distributions \n",
    "    prob_df['Probabilites_old'] = training_prob_dist\n",
    "    prob_df_new['Probabilites_new'] = new_prob_dist\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # convert the data into 5 bins based on the training data and get relative frequencies for each of those bins for\n",
    "    # training and new days data\n",
    "    bins = pd.qcut(prob_df['Probabilites_old'], 5,retbins=True, duplicates='drop')[1]\n",
    "    s1 = pd.cut(prob_df['Probabilites_old'], bins).value_counts() / len(prob_df['Probabilites_old'])\n",
    "    s2 = pd.cut(prob_df_new['Probabilites_new'], bins).value_counts() / len(prob_df_new['Probabilites_new'])\n",
    "    PSI_df = pd.concat([s1, s2], axis=1)\n",
    "    #PSI_df=PSI_df.replace(0,.0001)\n",
    "    if len(prob_df_new['Probabilites_new'].unique())<6:\n",
    "        print('-----------Model Issue Unique Probabilites fall into less than 3 buckets-------------')\n",
    "        print('new probas')\n",
    "        print(prob_df_new['Probabilites_new'].unique())\n",
    "        print('new old')\n",
    "        print(prob_df_new['Probabilites_old'].unique())\n",
    "    \n",
    "    # Caluclate the PSI metric\n",
    "    PSI_calc = ((PSI_df['Probabilites_new']- PSI_df['Probabilites_old']) * np.log(PSI_df['Probabilites_new']/PSI_df['Probabilites_old'])).sum()\n",
    "    \n",
    "    # retrain if the calculated metric is large than the threshold \n",
    "    if (PSI_calc> PSI_Threshold) and (day > 2):\n",
    "        print('        Drift Detected using: Probability Comparision')\n",
    "        print('        ',PSI_calc)\n",
    "        new_model,x,y,min_max_scaler = re_training(data,sampeling_ratio,model_dictionary[1],model_dictionary[0],model_dictionary[10])\n",
    "        \n",
    "        metric_df = pd.DataFrame([[day,sim,1,concept_drift]], columns=CD_tracked)\n",
    "        new_df = pd.concat([model_dictionary[9], metric_df],ignore_index=True)\n",
    "        \n",
    "\n",
    "        preds = new_model.predict(x)\n",
    "        pred_prob = new_model.predict_proba(x)\n",
    "        res = optimize_cost(Cost_FN, Cost_FP, pred_prob, y)\n",
    "        #new_train_metric = f1_score(y, preds)\n",
    "        new_train_metric = res[2]\n",
    "        new_opt_prob = res[0]\n",
    "        \n",
    "        model_dictionary[9] = new_df\n",
    "        model_dictionary[0] = new_model\n",
    "        model_dictionary[3] = new_train_metric\n",
    "        model_dictionary[5] = pred_prob[:,1]\n",
    "        model_dictionary[7] = new_opt_prob\n",
    "        model_dictionary[11] = min_max_scaler\n",
    "        return(model_dictionary)\n",
    "    else:\n",
    "        metric_df = pd.DataFrame([[day,sim,0,concept_drift]], columns=CD_tracked)\n",
    "        new_df = pd.concat([model_dictionary[9], metric_df],ignore_index=True)\n",
    "        model_dictionary[9] = new_df\n",
    "        return(model_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "743b22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that predicts fraud and finds optimal probability to predict fraud given the assoicated FN and FP costs\n",
    "def model_metric_predict(model_object, x_test, y_test, day, simulation,Cost_FN,Cost_FP):\n",
    "    metrics = ['Day','Simulation','Recall','F1 Score','Cost-Savings','Accuracy','Precision','Num_FPs','Num_TPs']\n",
    "    probabilities = model_object[0].predict_proba(x_test)\n",
    "    predictions =  probabilities[:,1].copy()\n",
    "    \n",
    "    \n",
    "    prob = model_object[7]\n",
    "\n",
    "    predictions[predictions>=prob] = 1\n",
    "    predictions[predictions<prob] = 0\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    \n",
    "    Recall = recall_score(y_test, predictions)\n",
    "    F1_Score = f1_score(y_test, predictions)\n",
    "    Cost_Savings = cm[1,1]*Cost_FN - cm[0,1]*Cost_FP\n",
    "    Accuracy = accuracy_score(y_test, predictions)\n",
    "    Precision = precision_score(y_test, predictions)\n",
    "    Num_FPs = cm[0,1]\n",
    "    Num_TPs = cm[1,1]\n",
    "    metric_df = pd.DataFrame([[day,simulation, Recall,F1_Score,Cost_Savings,Accuracy,Precision,Num_FPs,Num_TPs]], columns=Metrics_tracked)\n",
    "    new_df = pd.concat([model_object[8], metric_df],ignore_index=True)\n",
    "    probabilities = model_object[0].predict_proba(x_test)\n",
    "    predictions =  probabilities[:,1].copy()\n",
    "    return(new_df,F1_Score,predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f68466b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function resets each model object for each strategy at the begining of each simulation\n",
    "def reset_model_dictionary(model_dictionary,Base_model_dict):\n",
    "    for key, model_dict in model_dictionary.items():\n",
    "        model_dictionary[key][0] = deepcopy(Base_model_dict[key][0])\n",
    "        model_dictionary[key][3] = deepcopy(Base_model_dict[key][3])\n",
    "        model_dictionary[key][4] = deepcopy(Base_model_dict[key][4])\n",
    "        model_dictionary[key][5] = deepcopy(Base_model_dict[key][5])\n",
    "        model_dictionary[key][7] = deepcopy(Base_model_dict[key][7])\n",
    "        model_dictionary[key][11] = deepcopy(Base_model_dict[key][11])\n",
    "        model_dictionary[key][12] = 0\n",
    "        model_dictionary[key][13] = deepcopy(Base_model_dict[key][13])\n",
    "        \n",
    "    return(model_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478897e2",
   "metadata": {},
   "source": [
    "# Monte Claro Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90753074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main simulation function that runs the simulation for X simulations and X days\n",
    "def Simulate_Fraud (req_cols, base_data, model_dictionary, Iterations, Days,drift_days,time_between_drift, \n",
    "                    drift_magnitude,Cost_FN, Cost_FP,Base_model_dict,df_min_max,df_add, Fraud_PCT=.01):\n",
    "    # iterate through each simulation\n",
    "    for sim in range(Iterations):\n",
    "        print('Starting New Simulation {} of {}'.format(sim,Iterations))\n",
    "        time_sense_drift = time_between_drift\n",
    "        concept_drift = 0\n",
    "        df_add['use'] = 0\n",
    "        # reset the model objects after each simulation\n",
    "        if sim > 0:\n",
    "            model_dictionary = reset_model_dictionary(model_dictionary,Base_model_dict)\n",
    "        \n",
    "        # iterate over each days simulating new data and collecting each strategies metrics\n",
    "        for day in range(Days):\n",
    "            print('Starting New Day {} of {}'.format(day,Days))\n",
    "            ### Creating New Data\n",
    "            if day == 0:\n",
    "                simulated_data = synth_data_lib_v2_extra_drift.create_new_dataset (base_data,20000,Fraud_PCT,'fraud_bool',df_min_max,df_add)\n",
    "                simulated_data['day']=day\n",
    "                new_sim_data = simulated_data.copy()\n",
    "            else:\n",
    "                new_base_data = simulated_data[simulated_data['day']==day-1]\n",
    "                new_base_data.drop('day',axis=1,inplace=True)\n",
    "            if ( day < 3 and day > 0):\n",
    "                new_sim_data = synth_data_lib_v2_extra_drift.create_new_dataset (new_base_data,20000,Fraud_PCT,'fraud_bool',df_min_max,df_add)\n",
    "                new_sim_data['day']=day\n",
    "            if day >= 3 :\n",
    "                if (day in drift_days and time_sense_drift<1):\n",
    "                    print('Inserting drift')\n",
    "                    concept_drift = 1\n",
    "                    time_sense_drift = time_between_drift\n",
    "                    new_sim_data = synth_data_lib_v2_extra_drift.create_new_dataset (new_base_data,20000,Fraud_PCT,'fraud_bool',df_min_max,df_add,True,drift_magnitude)\n",
    "                    new_sim_data['day']=day\n",
    "                else:\n",
    "                    new_sim_data = synth_data_lib_v2_extra_drift.create_new_dataset (new_base_data,20000,Fraud_PCT,'fraud_bool',df_min_max,df_add)\n",
    "                    new_sim_data['day']=day\n",
    "            \n",
    "            ### Store new Data in Simulation total data dataframe\n",
    "            if day != 0:\n",
    "                simulated_data = pd.concat([simulated_data,new_sim_data],ignore_index=True)\n",
    "\n",
    "            print('tot Records: ', len(new_sim_data))\n",
    "            print('frd records: ', len(new_sim_data[new_sim_data['fraud_bool']==1]))\n",
    "            # iterate through each model strategy gather metrics, detect concept drift and retrain if neccessary \n",
    "            for key, model_dict in model_dictionary.items():\n",
    "                \n",
    "                \n",
    "                \n",
    "                ### Pre Process New data\n",
    "                x_test, y_test, min_max_throwaawy = pre_processing(new_sim_data, req_cols, model_dict_obj=model_dict)\n",
    "                \n",
    "                ## predict on new data per model\n",
    "                \n",
    "                model_results,test_metric,new_prob_dist = model_metric_predict(model_dict, x_test, y_test, day, sim,Cost_FN,Cost_FP)\n",
    "                \n",
    "                ## Store results in res dataframe\n",
    "                model_dictionary[key][8]= model_results\n",
    "                \n",
    "                ## detect on new data for every detection strat & Retrain if CD Detected\n",
    "                if model_dict[2]=='Preformance':\n",
    "                    new_mod_dict = cd_detection_metric_comparison(test_metric, model_dict[3], .15 ,model_dict,simulated_data,day,sim,concept_drift,Cost_FN, Cost_FP)\n",
    "                    model_dictionary[key] = new_mod_dict\n",
    "                \n",
    "                if model_dict[2]=='Feature_dist':\n",
    "                    new_mod_dict = cd_detection_Feature_distribution(simulated_data, 12,model_dict,day,sim,concept_drift,Cost_FN, Cost_FP)\n",
    "                    model_dictionary[key] = new_mod_dict\n",
    "                    \n",
    "                if model_dict[2]=='PSI':\n",
    "                    new_mod_dict = cd_PSI(model_dict[5], new_prob_dist, .1,model_dict,simulated_data,day,sim,concept_drift,Cost_FN, Cost_FP)\n",
    "                    model_dictionary[key] = new_mod_dict\n",
    "                    \n",
    "\n",
    "            concept_drift = 0\n",
    "            time_sense_drift -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "321596bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intalize dataframe to hold a list of results per iteration for each strategy \n",
    "Metrics_tracked = ['Day','Simulation','Recall','F1 Score','Cost-Savings','Accuracy','Precision','Num_FPs','Num_TPs']\n",
    "metric_results = pd.DataFrame(columns = Metrics_tracked)\n",
    "\n",
    "# Intalize dataframe to hold a list of CD metrics to track \n",
    "CD_tracked = ['Day','Simulation','CD_Detected','CD_Occured']\n",
    "CD_results = pd.DataFrame(columns = CD_tracked)\n",
    "\n",
    "# define simulation metrics\n",
    "Number_of_simulations = 20\n",
    "Number_of_days = 20\n",
    "drift_days = [4,9,14]\n",
    "time_between_drift= 3\n",
    "drift_magnitude = 100\n",
    "# Define Optimization metrics for custom loss function\n",
    "cost_fp = 10\n",
    "cost_fn = median_fraud\n",
    "req_feats = list(x_train.columns)\n",
    "\n",
    "# Create Probability Distance dataframe object\n",
    "inds = list(range(Number_of_days))\n",
    "cols = req_feats.copy()\n",
    "cols.append('CD_Detected')\n",
    "prob_dist_df = pd.DataFrame(columns=req_feats,index = inds)\n",
    "\n",
    "# Prepare Base data to be simulated \n",
    "base_data = x_train.copy()\n",
    "req_cols_base = list(x_train.columns)\n",
    "base_data['fraud_bool']=y_train\n",
    "# Intalize dictionary that will hold list of neccessary vars for model instances for each strategy that is updated \n",
    "# within the simulation \n",
    "\n",
    "\n",
    "Detection_Strategies = ['Preformance','Feature_dist','PSI']\n",
    "Retraining_strategies = [1,2,3]\n",
    "\n",
    "# Model object data dictionary\n",
    "# model_dictionary['ModelName_DetectionStrategy_RetrainStategy'] = [#0 model_instace,\n",
    "                                                                    #1 re_train strat\n",
    "                                                                    #2 detection Srat\n",
    "                                                                    #3 training_metric,\n",
    "                                                                    #4 base_data,\n",
    "                                                                    #5 training_prob_dist\n",
    "                                                                    #6 model optimization parm dict\n",
    "                                                                    #7 fraud predict proba\n",
    "                                                                    #8 Model Results datframe\n",
    "                                                                    #9 CD Detection Result\n",
    "                                                                    #10 required_columns\n",
    "                                                                    #11 Normalization object used in training\n",
    "                                                                    #12 last_day_cd_detected\n",
    "                                                                    #13 Probability Distance]\n",
    "\n",
    "# Intalize the 9 strategy combinations per base model\n",
    "# SGD Strategies (creates the 9 strategy combinations for the SGD Model)\n",
    "model_dictionary = {}\n",
    "for d_strat in Detection_Strategies:\n",
    "    for r_strat in Retraining_strategies:\n",
    "        key = 'SGD_{}_{}'.format(d_strat,r_strat)\n",
    "        SGD_Preformance = metric_results.copy()\n",
    "        SGD_CD_pref = CD_results.copy()\n",
    "        SGD_prob_dist = prob_dist_df.copy()\n",
    "        new_mod = deepcopy(opt_SGD)\n",
    "        new_prob_opt_SGD = deepcopy(SGD_Savings_proba)\n",
    "        copy_min_max_scaler_base = deepcopy(min_max_scaler_base)\n",
    "        model_dictionary[key] = [new_mod, #0 model_instace,\n",
    "                                r_strat, #1 re_train strat\n",
    "                                d_strat, #2 detection strat\n",
    "                                SGD_Savings_F1,  #3 training_metric,\n",
    "                                None,  #4 base_data,\n",
    "                                probabilities_SGD,  #5 training_prob_dist\n",
    "                                SGD_parameters,  #6 model optimization parm dict\n",
    "                                new_prob_opt_SGD,  #7 fraud predict proba\n",
    "                                SGD_Preformance, #8 Model Results datframe\n",
    "                                SGD_CD_pref, #9 CD Detection Result\n",
    "                                req_cols_base, #10 required_columns\n",
    "                                copy_min_max_scaler_base,#11 Normalization object  \n",
    "                                0,#12 last_day_cd_detected\n",
    "                                SGD_prob_dist] #13 Probability Distance\n",
    "\n",
    "# GBC Strategies (creates the 9 strategy combinations for the SGD Model)\n",
    "for d_strat in Detection_Strategies:\n",
    "    for r_strat in Retraining_strategies:\n",
    "        key = 'GBC_{}_{}'.format(d_strat,r_strat)\n",
    "        GBC_Preformance = metric_results.copy()\n",
    "        GBC_CD_pref = CD_results.copy()\n",
    "        GBC_prob_dist = prob_dist_df.copy()\n",
    "        new_mod = deepcopy(opt_GBC)\n",
    "        new_prob_opt_GBC = deepcopy(GBC_Savings_proba)\n",
    "        copy_min_max_scaler_base = deepcopy(min_max_scaler_base)\n",
    "        model_dictionary[key] = [new_mod,\n",
    "                                r_strat,\n",
    "                                d_strat,\n",
    "                                GBC_Savings_F1,\n",
    "                                None,\n",
    "                                probabilities_GBC,\n",
    "                                GBC_parameters,\n",
    "                                new_prob_opt_GBC,\n",
    "                                GBC_Preformance,\n",
    "                                GBC_CD_pref,\n",
    "                                req_cols_base,\n",
    "                                copy_min_max_scaler_base,\n",
    "                                0,\n",
    "                                GBC_prob_dist]\n",
    "\n",
    "# Add in base models that will never re-train\n",
    "\n",
    "new_mod_SGD = deepcopy(opt_SGD)\n",
    "SGDcopy_min_max_scaler_base = deepcopy(min_max_scaler_base)\n",
    "model_dictionary['SGD_BASE_BASE'] = [new_mod_SGD, #0 model_instace,\n",
    "                                    None, #1 detection Srat, \n",
    "                                    None, #2 re_train strat\n",
    "                                    None,  #3 training_metric,\n",
    "                                    None,  #4 base_data,\n",
    "                                    None,  #5 training_prob_dist\n",
    "                                    None,  #6 model optimization parm dict\n",
    "                                    SGD_Savings_proba,  #7 fraud predict proba\n",
    "                                    SGD_Preformance, #8 Model Results datframe\n",
    "                                    None, #9 CD Detection Result\n",
    "                                    req_cols_base,#10 required_columns\n",
    "                                    SGDcopy_min_max_scaler_base,#11 Normalization object \n",
    "                                    None,\n",
    "                                    None] #12 last day cd detected\n",
    "\n",
    "copy_min_max_scaler_base = deepcopy(min_max_scaler_base)\n",
    "new_mod_GBC = deepcopy(opt_GBC)\n",
    "model_dictionary['GBC_BASE_BASE'] = [new_mod_GBC,\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    GBC_Savings_proba,\n",
    "                                    GBC_Preformance,\n",
    "                                    None,\n",
    "                                    req_cols_base,\n",
    "                                    copy_min_max_scaler_base,\n",
    "                                    None,\n",
    "                                    None]\n",
    "\n",
    "# Store the Numeric Min Max Values of original Data set\n",
    "org_cols = list(df_use.columns)\n",
    "df_min_max = pd.DataFrame(columns = org_cols, index= ['Min','Max'])\n",
    "\n",
    "for col in org_cols:\n",
    "    if df_use[col].dtype in ['int64', 'float64']:\n",
    "        df_min_max.loc['Min',col]=df_use[col].min()\n",
    "        df_min_max.loc['Max',col]=df_use[col].max()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8071b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get copy of orginal model object\n",
    "base_model_dic = deepcopy(model_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8e826d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting New Simulation 0 of 20\n",
      "Starting New Day 0 of 20\n",
      "tot Records:  20000\n",
      "frd records:  200\n",
      "Starting New Day 1 of 20\n",
      "tot Records:  20000\n",
      "frd records:  200\n",
      "Starting New Day 2 of 20\n",
      "tot Records:  20000\n",
      "frd records:  200\n",
      "Starting New Day 3 of 20\n",
      "tot Records:  20000\n",
      "frd records:  200\n",
      "Starting New Day 4 of 20\n",
      "Inserting drift\n",
      "tot Records:  20000\n",
      "frd records:  200\n",
      "        Drift Detected using: Metric Comparision\n",
      "        Current_Metric:  0.0\n",
      "        Train Metric:  0.22130013831258644\n",
      "        Drift Detected using: Metric Comparision\n",
      "        Current_Metric:  0.0\n",
      "        Train Metric:  0.22130013831258644\n",
      "        Drift Detected using: Metric Comparision\n",
      "        Current_Metric:  0.0\n",
      "        Train Metric:  0.22130013831258644\n",
      "        Drift Detected using: Feature Comparision\n",
      "        Drift Detected using: Feature Comparision\n",
      "        Drift Detected using: Feature Comparision\n",
      "        Drift Detected using: Metric Comparision\n",
      "        Current_Metric:  0.009302325581395349\n",
      "        Train Metric:  0.23876404494382023\n",
      "        Drift Detected using: Metric Comparision\n",
      "        Current_Metric:  0.009302325581395349\n",
      "        Train Metric:  0.23876404494382023\n",
      "        Drift Detected using: Metric Comparision\n",
      "        Current_Metric:  0.009302325581395349\n",
      "        Train Metric:  0.23876404494382023\n",
      "        Drift Detected using: Feature Comparision\n",
      "        Drift Detected using: Feature Comparision\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# run simulation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mSimulate_Fraud\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNumber_of_simulations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNumber_of_days\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdrift_days\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtime_between_drift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrift_magnitude\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcost_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcost_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbase_model_dic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_min_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_add\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFraud_PCT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mSimulate_Fraud\u001b[1;34m(req_cols, base_data, model_dictionary, Iterations, Days, drift_days, time_between_drift, drift_magnitude, Cost_FN, Cost_FP, Base_model_dict, df_min_max, df_add, Fraud_PCT)\u001b[0m\n\u001b[0;32m     63\u001b[0m     model_dictionary[key] \u001b[38;5;241m=\u001b[39m new_mod_dict\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_dict[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature_dist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m     new_mod_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcd_detection_Feature_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulated_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mday\u001b[49m\u001b[43m,\u001b[49m\u001b[43msim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconcept_drift\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCost_FN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCost_FP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     model_dictionary[key] \u001b[38;5;241m=\u001b[39m new_mod_dict\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_dict[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPSI\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mcd_detection_Feature_distribution\u001b[1;34m(all_data, threshold_criteria, model_dictionary, day, sim, concept_drift, Cost_FN, Cost_FP, sampeling_ratio)\u001b[0m\n\u001b[0;32m    163\u001b[0m new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([model_dictionary[\u001b[38;5;241m9\u001b[39m], metric_df],ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# retrain and return model and sclaer object used to retrain\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m new_model,x,y,min_max_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mre_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43msampeling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_dictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_dictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_dictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# obtain new model object values and assign them to the model object\u001b[39;00m\n\u001b[0;32m    169\u001b[0m preds \u001b[38;5;241m=\u001b[39m new_model\u001b[38;5;241m.\u001b[39mpredict(x)\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mre_training\u001b[1;34m(data, sampeling_ratio, strategy, base_model, req_cols)\u001b[0m\n\u001b[0;32m     28\u001b[0m     undersample \u001b[38;5;241m=\u001b[39m RandomUnderSampler(sampling_strategy\u001b[38;5;241m=\u001b[39msampeling_ratio)\n\u001b[0;32m     29\u001b[0m     x_sampled, y_sampled \u001b[38;5;241m=\u001b[39m undersample\u001b[38;5;241m.\u001b[39mfit_resample(x_train,y_train)\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(base_model,x_train,y_train,min_max_scaler)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# retrain on 3 days worth of data but give more weight to most recent day\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    608\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    609\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    610\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    611\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    612\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 615\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:257\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    254\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    256\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 257\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    260\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    261\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    262\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    270\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run simulation\n",
    "Simulate_Fraud (req_feats, df_use, model_dictionary, Number_of_simulations, Number_of_days,drift_days, \n",
    "                    time_between_drift, drift_magnitude,cost_fn,cost_fp,base_model_dic,df_min_max,df_add,Fraud_PCT = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for metrics and Concept Drift Metrics to csv fiels\n",
    "for key, model_dict in model_dictionary.items():\n",
    "    res_name = key+'_metrics.csv'\n",
    "    cd_name =  key+'_Concept_Drift.csv'\n",
    "    model_dict[8].to_csv(res_name)\n",
    "    try:\n",
    "        model_dict[9].to_csv(cd_name)\n",
    "    except:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c540173",
   "metadata": {},
   "outputs": [],
   "source": [
    "Detection_Strategies = ['Preformance','Feature_dist','PSI']\n",
    "Retraining_strategies = ['1','2','3']\n",
    "Model_Types = ['SGD','GBC']\n",
    "\n",
    "start = 0\n",
    "\n",
    "# for each csv file combine the metrics and concept drift of each strategy into one dataframe with all values\n",
    "for d_strat in Detection_Strategies:\n",
    "    for r_strat in Retraining_strategies:\n",
    "        for mod_name in Model_Types:\n",
    "            f_name_metrics = mod_name+'_'+d_strat+'_'+r_strat+'_metrics.csv'\n",
    "            f_name_CD = mod_name+'_'+d_strat+'_'+r_strat+'_Concept_Drift.csv'\n",
    "            Strategy = mod_name+'_'+d_strat+'_'+r_strat\n",
    "            \n",
    "            if start == 0:       \n",
    "                ALL_df_metric = pd.read_csv(f_name_metrics)\n",
    "                ALL_df_metric['Strategy']=Strategy\n",
    "                ALL_df_metric['Model'] = mod_name\n",
    "                ALL_df_metric['Retrain_strategy'] = r_strat\n",
    "                ALL_df_metric['Detection_Strategy'] = d_strat\n",
    "                \n",
    "                ALL_df_cd = pd.read_csv(f_name_CD)\n",
    "                ALL_df_cd['Strategy']=Strategy\n",
    "                ALL_df_cd['Model'] = mod_name\n",
    "                ALL_df_cd['Retrain_strategy'] = r_strat\n",
    "                ALL_df_cd['Detection_Strategy'] = d_strat\n",
    "                start = 1\n",
    "            else:\n",
    "                df_metric = pd.read_csv(f_name_metrics)\n",
    "                df_metric['Strategy']=Strategy\n",
    "                df_metric['Model'] = mod_name\n",
    "                df_metric['Retrain_strategy'] = r_strat\n",
    "                df_metric['Detection_Strategy'] = d_strat\n",
    "                ALL_df_metric = pd.concat([ALL_df_metric, df_metric])\n",
    "                \n",
    "                df_cd = pd.read_csv(f_name_CD)\n",
    "                df_cd['Strategy']=Strategy\n",
    "                df_cd['Model'] = mod_name\n",
    "                df_cd['Retrain_strategy'] = r_strat\n",
    "                df_cd['Detection_Strategy'] = d_strat\n",
    "                ALL_df_cd = pd.concat([ALL_df_cd, df_cd])\n",
    "\n",
    "# store combined dataframe into one dataframe                \n",
    "All_Values_df = ALL_df_metric.merge(ALL_df_cd,on= ['Day','Simulation','Strategy','Model','Retrain_strategy','Detection_Strategy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c875ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to file\n",
    "All_Values_df.to_csv('alll_vals-10-31-2023-extra-drift.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1107c872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>Day</th>\n",
       "      <th>Simulation</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Cost-Savings</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Num_FPs</th>\n",
       "      <th>Num_TPs</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Model</th>\n",
       "      <th>Retrain_strategy</th>\n",
       "      <th>Detection_Strategy</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>CD_Detected</th>\n",
       "      <th>CD_Occured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>317.566172</td>\n",
       "      <td>0.98850</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>55</td>\n",
       "      <td>25</td>\n",
       "      <td>SGD_Preformance_1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1</td>\n",
       "      <td>Preformance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>686.376760</td>\n",
       "      <td>0.98985</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>SGD_Preformance_1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1</td>\n",
       "      <td>Preformance</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>312.863525</td>\n",
       "      <td>0.98860</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>52</td>\n",
       "      <td>24</td>\n",
       "      <td>SGD_Preformance_1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1</td>\n",
       "      <td>Preformance</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.169884</td>\n",
       "      <td>393.458232</td>\n",
       "      <td>0.98925</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>37</td>\n",
       "      <td>22</td>\n",
       "      <td>SGD_Preformance_1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1</td>\n",
       "      <td>Preformance</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-500.000000</td>\n",
       "      <td>0.98750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>SGD_Preformance_1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1</td>\n",
       "      <td>Preformance</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>395</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>-505.297353</td>\n",
       "      <td>0.98735</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>GBC_PSI_3</td>\n",
       "      <td>GBC</td>\n",
       "      <td>3</td>\n",
       "      <td>PSI</td>\n",
       "      <td>395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>396</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>-465.297353</td>\n",
       "      <td>0.98755</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>GBC_PSI_3</td>\n",
       "      <td>GBC</td>\n",
       "      <td>3</td>\n",
       "      <td>PSI</td>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7197</th>\n",
       "      <td>397</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-380.000000</td>\n",
       "      <td>0.98810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>GBC_PSI_3</td>\n",
       "      <td>GBC</td>\n",
       "      <td>3</td>\n",
       "      <td>PSI</td>\n",
       "      <td>397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>398</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-410.000000</td>\n",
       "      <td>0.98795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>GBC_PSI_3</td>\n",
       "      <td>GBC</td>\n",
       "      <td>3</td>\n",
       "      <td>PSI</td>\n",
       "      <td>398</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7199</th>\n",
       "      <td>399</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>-295.297353</td>\n",
       "      <td>0.98840</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>GBC_PSI_3</td>\n",
       "      <td>GBC</td>\n",
       "      <td>3</td>\n",
       "      <td>PSI</td>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7200 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0_x  Day  Simulation  Recall  F1 Score  Cost-Savings  Accuracy  \\\n",
       "0                0    0           0   0.125  0.178571    317.566172   0.98850   \n",
       "1                1    1           0   0.145  0.222222    686.376760   0.98985   \n",
       "2                2    2           0   0.120  0.173913    312.863525   0.98860   \n",
       "3                3    3           0   0.110  0.169884    393.458232   0.98925   \n",
       "4                4    4           0   0.000  0.000000   -500.000000   0.98750   \n",
       "...            ...  ...         ...     ...       ...           ...       ...   \n",
       "7195           395   15          19   0.005  0.007843   -505.297353   0.98735   \n",
       "7196           396   16          19   0.005  0.007968   -465.297353   0.98755   \n",
       "7197           397   17          19   0.000  0.000000   -380.000000   0.98810   \n",
       "7198           398   18          19   0.000  0.000000   -410.000000   0.98795   \n",
       "7199           399   19          19   0.005  0.008547   -295.297353   0.98840   \n",
       "\n",
       "      Precision  Num_FPs  Num_TPs           Strategy Model Retrain_strategy  \\\n",
       "0      0.312500       55       25  SGD_Preformance_1   SGD                1   \n",
       "1      0.475410       32       29  SGD_Preformance_1   SGD                1   \n",
       "2      0.315789       52       24  SGD_Preformance_1   SGD                1   \n",
       "3      0.372881       37       22  SGD_Preformance_1   SGD                1   \n",
       "4      0.000000       50        0  SGD_Preformance_1   SGD                1   \n",
       "...         ...      ...      ...                ...   ...              ...   \n",
       "7195   0.018182       54        1          GBC_PSI_3   GBC                3   \n",
       "7196   0.019608       50        1          GBC_PSI_3   GBC                3   \n",
       "7197   0.000000       38        0          GBC_PSI_3   GBC                3   \n",
       "7198   0.000000       41        0          GBC_PSI_3   GBC                3   \n",
       "7199   0.029412       33        1          GBC_PSI_3   GBC                3   \n",
       "\n",
       "     Detection_Strategy  Unnamed: 0_y  CD_Detected  CD_Occured  \n",
       "0           Preformance             0            0           0  \n",
       "1           Preformance             1            0           0  \n",
       "2           Preformance             2            0           0  \n",
       "3           Preformance             3            0           0  \n",
       "4           Preformance             4            1           1  \n",
       "...                 ...           ...          ...         ...  \n",
       "7195                PSI           395            0           0  \n",
       "7196                PSI           396            0           0  \n",
       "7197                PSI           397            0           0  \n",
       "7198                PSI           398            0           0  \n",
       "7199                PSI           399            0           0  \n",
       "\n",
       "[7200 rows x 17 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67951a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82559214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
